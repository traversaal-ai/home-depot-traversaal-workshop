{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Setup: BigQuery Data Ingestion (Safe Version)\n\nThis notebook safely sets up the BigQuery tables required for the Delivery Intelligence Agent workshop.\n\n## Features\n- ‚úÖ Safe to run multiple times (idempotent)\n- ‚úÖ Checks if tables exist before creating\n- ‚úÖ Option to append or replace data\n- ‚úÖ Clear status messages throughout\n- ‚úÖ Automatic fallback to CSV loading if pyarrow is not available\n\n## Requirements\n- Google Cloud Project with BigQuery enabled\n- Authentication set up (`gcloud auth application-default login`)\n- Python packages: pandas, google-cloud-bigquery\n- Optional but recommended: pyarrow (for faster uploads)\n\n## Overview\n\nWe'll create 3 tables from the CSV data:\n1. **delivery_orders** - Main table with core delivery information\n2. **delivery_notes** - Customer and historical notes (separated due to long text)\n3. **delivery_products** - Product SKUs (normalized from comma-delimited list)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your preferences here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nPROJECT_ID = \"traversaal-research\"  # Change this to your project\nDATASET_ID = \"delivery_intelligence\"\nLOCATION = \"US\"  # Changed from us-central1 to US for BigQuery\n\n# Data loading preferences\nREPLACE_EXISTING_DATA = True  # Set to False to skip tables that already exist\nSHOW_SAMPLE_DATA = True      # Set to False to skip sample queries at the end"
  },
  {
   "cell_type": "markdown",
   "source": "## Clean Slate Setup\n\nBefore starting the workshop, let's clean up any existing output files from previous runs to ensure you see the actual pipeline execution:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport glob\n\n# Configuration\nCLEANUP_OUTPUT_FILES = True  # Set to False to skip cleanup\n\nif CLEANUP_OUTPUT_FILES:\n    print(\"üßπ Cleaning up existing output files from exercises...\")\n    print(\"=\" * 60)\n    \n    # Define output files to clean up\n    output_files = [\n        # Exercise 1 outputs\n        \"../exercise_1_data_collection/collected_order_data.json\",\n        \n        # Exercise 2 outputs\n        \"../exercise_2_risk_assessment/risk_assessment_output.json\",\n        \"../risk_assessment_output.json\",  # In case it was saved in parent directory\n        \"../exercise_2_risk_assessment/weather_mcp_server_demo.py\",  # Demo file created by notebook\n        \n        # Exercise 3 outputs\n        \"../exercise_3_product_intelligence/product_intelligence_output.json\",\n        \n        # Exercise 4 outputs\n        \"../exercise_4_communication_generation/communication_output.json\",\n        \n        # Exercise 5 outputs\n        \"../exercise_5_final_integration/delivery_case_card.json\",\n        \"../exercise_5_final_integration/delivery_case_card.md\",\n        \"../exercise_5_final_integration/collected_order_data.json\",\n        \"../exercise_5_final_integration/risk_assessment_output.json\",\n        \"../exercise_5_final_integration/product_intelligence_output.json\",\n        \"../exercise_5_final_integration/communication_output.json\",\n    ]\n    \n    # Also clean up any Jupyter checkpoint files\n    checkpoint_patterns = [\n        \"../*/.ipynb_checkpoints/*\",\n        \"../*/*/.ipynb_checkpoints/*\"\n    ]\n    \n    cleaned_count = 0\n    \n    # Clean specific output files\n    for file_path in output_files:\n        if os.path.exists(file_path):\n            try:\n                os.remove(file_path)\n                print(f\"‚úì Removed: {os.path.basename(file_path)}\")\n                cleaned_count += 1\n            except Exception as e:\n                print(f\"‚ö†Ô∏è  Could not remove {file_path}: {e}\")\n        else:\n            # File doesn't exist, which is fine\n            pass\n    \n    # Clean checkpoint files\n    for pattern in checkpoint_patterns:\n        checkpoint_files = glob.glob(pattern)\n        for checkpoint in checkpoint_files:\n            try:\n                os.remove(checkpoint)\n                print(f\"‚úì Removed checkpoint: {os.path.basename(checkpoint)}\")\n                cleaned_count += 1\n            except Exception as e:\n                print(f\"‚ö†Ô∏è  Could not remove {checkpoint}: {e}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    if cleaned_count > 0:\n        print(f\"‚úÖ Cleanup complete! Removed {cleaned_count} files.\")\n        print(\"\\nüí° The exercises will now generate fresh outputs when you run them.\")\n    else:\n        print(\"‚úÖ No files to clean up - starting with a clean environment!\")\n        \n    print(\"\\nüìù Note: This cleanup ensures you see the actual pipeline execution\")\n    print(\"   rather than cached results from previous runs.\")\nelse:\n    print(\"‚ÑπÔ∏è  Cleanup skipped (CLEANUP_OUTPUT_FILES=False)\")\n    print(\"   Existing output files will be preserved.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install required packages if not already installed\nimport subprocess\nimport sys\n\ndef install_package(package):\n    \"\"\"Install a package using pip\"\"\"\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n# Check and install required packages\npackages_to_install = {\n    'pyarrow': 'pyarrow>=6.0.0',\n    'google-cloud-bigquery': 'google-cloud-bigquery[pandas]',\n    'pandas': 'pandas>=1.3.0',\n    'db-dtypes': 'db-dtypes'  # Required for BigQuery datetime handling\n}\n\nprint(\"Checking required packages...\")\nfor package_name, package_spec in packages_to_install.items():\n    try:\n        __import__(package_name.replace('-', '_'))\n        print(f\"‚úÖ {package_name} is already installed\")\n    except ImportError:\n        print(f\"üì¶ Installing {package_spec}...\")\n        if install_package(package_spec):\n            print(f\"‚úÖ Successfully installed {package_name}\")\n        else:\n            print(f\"‚ùå Failed to install {package_name} - please install manually\")\n            print(f\"   Run: pip install {package_spec}\")\n\nprint(\"\\n‚úÖ Package check complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Install Required Dependencies\n\nMake sure you have the necessary packages installed:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import Conflict, NotFound\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up environment\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "# Initialize BigQuery client\n",
    "try:\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    print(f\"‚úÖ Connected to project: {PROJECT_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to BigQuery: {e}\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"1. You have the correct project ID\")\n",
    "    print(\"2. You are authenticated (run 'gcloud auth application-default login')\")\n",
    "    print(\"3. You have BigQuery API enabled\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def table_exists(client, dataset_id, table_id):\n    \"\"\"Check if a table exists in BigQuery\"\"\"\n    try:\n        table_ref = f\"{PROJECT_ID}.{dataset_id}.{table_id}\"\n        client.get_table(table_ref)\n        return True\n    except NotFound:\n        return False\n    except Exception as e:\n        print(f\"Error checking table {table_id}: {e}\")\n        return False\n\ndef safe_load_table(client, df, table_full_id, table_name):\n    \"\"\"Safely load data to BigQuery table\"\"\"\n    # Check if table exists\n    dataset_id = table_full_id.split('.')[1]\n    table_id = table_full_id.split('.')[2]\n    \n    if table_exists(client, dataset_id, table_id):\n        if REPLACE_EXISTING_DATA:\n            print(f\"‚ö†Ô∏è  Table {table_name} exists - replacing data...\")\n            write_disposition = \"WRITE_TRUNCATE\"\n        else:\n            print(f\"‚ÑπÔ∏è  Table {table_name} exists - skipping (REPLACE_EXISTING_DATA=False)\")\n            return True\n    else:\n        print(f\"üìä Creating new table {table_name}...\")\n        write_disposition = \"WRITE_TRUNCATE\"\n    \n    # Load data\n    job_config = bigquery.LoadJobConfig(\n        write_disposition=write_disposition,\n        autodetect=True,\n    )\n    \n    try:\n        # Try pandas method first (requires pyarrow)\n        try:\n            job = client.load_table_from_dataframe(df, table_full_id, job_config=job_config)\n            job.result()  # Wait for job to complete\n            print(f\"‚úÖ Successfully loaded {len(df):,} rows into {table_name}\")\n            return True\n        except Exception as pandas_error:\n            if \"pyarrow\" in str(pandas_error):\n                print(\"‚ö†Ô∏è  Pyarrow not available, using CSV method instead...\")\n                \n                # Alternative: Save to temporary CSV and load\n                import tempfile\n                import os\n                \n                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp_file:\n                    df.to_csv(tmp_file.name, index=False)\n                    tmp_path = tmp_file.name\n                \n                # Load from CSV file\n                with open(tmp_path, 'rb') as source_file:\n                    job = client.load_table_from_file(\n                        source_file, \n                        table_full_id, \n                        job_config=job_config\n                    )\n                    job.result()\n                \n                # Clean up temp file\n                os.unlink(tmp_path)\n                \n                print(f\"‚úÖ Successfully loaded {len(df):,} rows into {table_name} (via CSV)\")\n                return True\n            else:\n                raise pandas_error\n                \n    except Exception as e:\n        print(f\"‚ùå Error loading {table_name}: {e}\")\n        return False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset if it doesn't exist\n",
    "dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = LOCATION\n",
    "dataset.description = \"Delivery Intelligence Workshop Data\"\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print(f\"‚úÖ Created new dataset: {dataset_id}\")\n",
    "except Conflict:\n",
    "    print(f\"‚ÑπÔ∏è  Dataset {dataset_id} already exists - using existing dataset\")\n",
    "    dataset = client.get_dataset(dataset_id)\n",
    "    \n",
    "    # List existing tables\n",
    "    tables = list(client.list_tables(dataset))\n",
    "    if tables:\n",
    "        print(f\"\\nüìã Existing tables in dataset:\")\n",
    "        for table in tables:\n",
    "            table_ref = client.get_table(table.reference)\n",
    "            print(f\"   - {table.table_id} ({table_ref.num_rows:,} rows)\")\n",
    "    else:\n",
    "        print(\"   (No existing tables)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main data CSV\n",
    "csv_path = '../db-data-csv/main-data.csv'\n",
    "\n",
    "try:\n",
    "    print(f\"üìÅ Loading CSV from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå CSV file not found at {csv_path}\")\n",
    "    print(\"\\nPlease ensure the CSV file exists at the correct path.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CSV: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Preparing data...\")\n",
    "\n",
    "# Fix any potential encoding issues in column names\n",
    "df.columns = [col.replace('\\ufeff', '') for col in df.columns]\n",
    "\n",
    "# Convert date columns to proper datetime format\n",
    "date_columns = ['SCHEDULED_DELIVERY_DATE', 'DELIVERY_CREATE_DATE']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"   ‚úì Converted {col} to datetime\")\n",
    "\n",
    "# Convert time columns to string format for BigQuery\n",
    "time_columns = ['WINDOW_START', 'WINDOW_END']\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "        print(f\"   ‚úì Converted {col} to string\")\n",
    "\n",
    "# Handle boolean columns\n",
    "bool_columns = ['SPECIAL_ORDER', 'UNATTENDED_FLAG', 'PRO_XTRA_MEMBER', \n",
    "                'MANAGED_ACCOUNT', 'COMMERCIAL_ADDRESS_FLAG']\n",
    "for col in bool_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map({'TRUE': True, 'FALSE': False, True: True, False: False})\n",
    "        print(f\"   ‚úì Converted {col} to boolean\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: delivery_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for the main delivery_orders table\n",
    "delivery_orders_columns = [\n",
    "    'DATA_ID', 'MARKET', 'SCHEDULED_DELIVERY_DATE', 'DELIVERY_CREATE_DATE',\n",
    "    'VEHICLE_TYPE', 'CUSTOMER_ORDER_NUMBER', 'WORK_ORDER_NUMBER', 'SPECIAL_ORDER',\n",
    "    'FLOC', 'FLOC_TYPE', 'SERVICE_TYPE', 'UNATTENDED_FLAG', 'WINDOW_START',\n",
    "    'WINDOW_END', 'QUANTITY', 'VOLUME_CUBEFT', 'WEIGHT', 'PALLET',\n",
    "    'FLOC_DELIVERY_ATTEMPTS_LAST_15_DAYS', 'FLOC_OTC_FAILURES_LAST_15_DAYS',\n",
    "    'FLOC_OTC_FAILURE_PCT_LAST_15_DAYS', 'DLVRY_RISK_DECILE', 'DLVRY_RISK_BUCKET',\n",
    "    'DLVRY_RISK_PERCENTILE', 'DLVRY_RISK_TOP_FEATURE', 'DESTINATION_ADDRESS',\n",
    "    'STREET_VIEW_URL', 'COMMERCIAL_ADDRESS_FLAG', 'BUSINESS_HOURS',\n",
    "    'STRT_VW_IMG_DSCRPTN', 'WTHR_CATEGORY', 'PRECIPITATION', 'CUSTOMER_NAME',\n",
    "    'PRO_XTRA_MEMBER', 'MANAGED_ACCOUNT', 'OSR_NAME', 'RI_GENERATE_DATETIME'\n",
    "]\n",
    "\n",
    "# Create delivery_orders dataframe\n",
    "print(\"\\nüìä Processing delivery_orders table...\")\n",
    "df_delivery_orders = df[delivery_orders_columns].copy()\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.delivery_orders\"\n",
    "\n",
    "# Load to BigQuery\n",
    "safe_load_table(client, df_delivery_orders, table_id, \"delivery_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2: delivery_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for the delivery_notes table\n",
    "delivery_notes_columns = [\n",
    "    'DATA_ID', 'CUSTOMER_NOTES', 'CUSTOMER_NOTES_LLM_SUMMARY',\n",
    "    'HISTORIC_NOTES_LLM_SUMMARY', 'CUSTOMER_NOTES_KEY_WORDS',\n",
    "    'HISTORIC_NOTES_W_LABELS'\n",
    "]\n",
    "\n",
    "# Create delivery_notes dataframe\n",
    "print(\"\\nüìä Processing delivery_notes table...\")\n",
    "df_delivery_notes = df[delivery_notes_columns].copy()\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.delivery_notes\"\n",
    "\n",
    "# Load to BigQuery\n",
    "safe_load_table(client, df_delivery_notes, table_id, \"delivery_notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: delivery_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SKU data - split comma-delimited lists into separate rows\n",
    "print(\"\\nüìä Processing delivery_products table...\")\n",
    "print(\"   Splitting comma-delimited SKUs into individual rows...\")\n",
    "\n",
    "product_rows = []\n",
    "skipped_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    data_id = row['DATA_ID']\n",
    "    sku_desc = row['SKU_DESCRIPTION']\n",
    "    \n",
    "    if pd.notna(sku_desc) and sku_desc:\n",
    "        # Split by ' ,|' which seems to be the delimiter\n",
    "        skus = [sku.strip() for sku in str(sku_desc).split(' ,|')]\n",
    "        \n",
    "        for sku in skus:\n",
    "            if sku:  # Only add non-empty SKUs\n",
    "                product_rows.append({\n",
    "                    'DATA_ID': data_id,\n",
    "                    'SKU_DESCRIPTION': sku\n",
    "                })\n",
    "    else:\n",
    "        skipped_count += 1\n",
    "\n",
    "# Create products dataframe\n",
    "df_delivery_products = pd.DataFrame(product_rows)\n",
    "print(f\"   Created {len(df_delivery_products):,} product rows from {len(df):,} deliveries\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"   Skipped {skipped_count} deliveries with no products\")\n",
    "\n",
    "# Load to BigQuery\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.delivery_products\"\n",
    "safe_load_table(client, df_delivery_products, table_id, \"delivery_products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the dataset with their stats\n",
    "print(\"\\nüìä Final table summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tables = list(client.list_tables(dataset))\n",
    "total_size = 0\n",
    "total_rows = 0\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        table_ref = client.get_table(table.reference)\n",
    "        size_mb = table_ref.num_bytes / 1024 / 1024\n",
    "        total_size += size_mb\n",
    "        total_rows += table_ref.num_rows\n",
    "        \n",
    "        print(f\"\\nüìå {table.table_id}\")\n",
    "        print(f\"   Rows: {table_ref.num_rows:,}\")\n",
    "        print(f\"   Size: {size_mb:.2f} MB\")\n",
    "        print(f\"   Columns: {len(table_ref.schema)}\")\n",
    "        print(f\"   Created: {table_ref.created}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error getting info for {table.table_id}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Total: {len(tables)} tables, {total_rows:,} rows, {total_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Queries (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SAMPLE_DATA:\n",
    "    print(\"\\nüîç Running sample queries...\\n\")\n",
    "    \n",
    "    # Query 1: Sample from delivery_orders\n",
    "    try:\n",
    "        query1 = f\"\"\"\n",
    "        SELECT \n",
    "            DATA_ID,\n",
    "            CUSTOMER_ORDER_NUMBER,\n",
    "            SCHEDULED_DELIVERY_DATE,\n",
    "            VEHICLE_TYPE,\n",
    "            DLVRY_RISK_BUCKET\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.delivery_orders`\n",
    "        LIMIT 3\n",
    "        \"\"\"\n",
    "        df_sample = client.query(query1).to_dataframe()\n",
    "        print(\"üìã Sample from delivery_orders:\")\n",
    "        print(df_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying delivery_orders: {e}\")\n",
    "    \n",
    "    # Query 2: Check for notes\n",
    "    try:\n",
    "        query2 = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_deliveries,\n",
    "            COUNTIF(CUSTOMER_NOTES IS NOT NULL) as deliveries_with_notes\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.delivery_notes`\n",
    "        \"\"\"\n",
    "        df_notes_stats = client.query(query2).to_dataframe()\n",
    "        print(\"\\nüìã Notes statistics:\")\n",
    "        print(df_notes_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying delivery_notes: {e}\")\n",
    "    \n",
    "    # Query 3: Product counts\n",
    "    try:\n",
    "        query3 = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT DATA_ID) as unique_deliveries,\n",
    "            COUNT(*) as total_products,\n",
    "            ROUND(COUNT(*) / COUNT(DISTINCT DATA_ID), 2) as avg_products_per_delivery\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.delivery_products`\n",
    "        \"\"\"\n",
    "        df_product_stats = client.query(query3).to_dataframe()\n",
    "        print(\"\\nüìã Product statistics:\")\n",
    "        print(df_product_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying delivery_products: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Sample queries skipped (SHOW_SAMPLE_DATA=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ Setup Complete!\n",
    "\n",
    "The following tables are now available in BigQuery:\n",
    "1. **delivery_orders** - Main delivery data\n",
    "2. **delivery_notes** - Customer and historical notes\n",
    "3. **delivery_products** - Individual product SKUs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Update Exercise notebooks** to use these tables:\n",
    "   - `{PROJECT_ID}.{DATASET_ID}.delivery_orders`\n",
    "   - `{PROJECT_ID}.{DATASET_ID}.delivery_notes`\n",
    "   - `{PROJECT_ID}.{DATASET_ID}.delivery_products`\n",
    "\n",
    "2. **Common queries for agents**:\n",
    "   ```sql\n",
    "   -- Get complete order info\n",
    "   SELECT o.*, n.CUSTOMER_NOTES\n",
    "   FROM `delivery_orders` o\n",
    "   LEFT JOIN `delivery_notes` n USING(DATA_ID)\n",
    "   WHERE o.DATA_ID = ?\n",
    "   \n",
    "   -- Get products for an order\n",
    "   SELECT SKU_DESCRIPTION \n",
    "   FROM `delivery_products`\n",
    "   WHERE DATA_ID = ?\n",
    "   ```\n",
    "\n",
    "3. **Test with workshop notebooks** to ensure agents can query effectively\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "- Check the PROJECT_ID is correct\n",
    "- Ensure you have BigQuery permissions\n",
    "- Verify the CSV file path is correct\n",
    "- Check REPLACE_EXISTING_DATA setting if tables already exist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}