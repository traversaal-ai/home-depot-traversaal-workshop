{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56a0b07",
   "metadata": {},
   "source": [
    "# Exercise 1: Orchestrating Data for Delivery Intelligence\n",
    "\n",
    "This is the first part in a series of exercises where you'll build a Delivery Route Intelligence Agentâ€”designed to help general office associates (GOAs) reduce avoidable delivery failures.\n",
    "\n",
    "In this exercise, you'll learn how to:\n",
    "- Query BigQuery tables using ADK agents\n",
    "- Orchestrate multiple agents working together\n",
    "- Pass data between agents using `output_key`\n",
    "- Build a pipeline that works with a specific order number\n",
    "\n",
    "By the end, you'll have a working data collection pipeline that fetches comprehensive delivery information for any order in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ha5x4ddm8io",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Order Selection\n",
    "\n",
    "Choose which order to analyze by setting the ORDER_NUMBER below. This order will be used throughout all exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6y7mtd21shl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT YOUR ORDER NUMBER HERE\n",
    "ORDER_NUMBER = \"CG92094171\"  # Change this to analyze a different order\n",
    "\n",
    "print(f\"ðŸ“¦ Order selected: {ORDER_NUMBER}\")\n",
    "print(\"This order will be used throughout the workshop exercises.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf82229",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ae03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress ALL warnings for clean output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Configure Google Cloud environment\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"traversaal-research\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\"\n",
    "\n",
    "# BigQuery configuration\n",
    "DATASET_ID = \"delivery_intelligence\"\n",
    "PROJECT_ID = \"traversaal-research\"\n",
    "\n",
    "print(f\"âœ… Environment configured\")\n",
    "print(f\"ðŸ“Š Using BigQuery dataset: {PROJECT_ID}.{DATASET_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f07613",
   "metadata": {},
   "source": [
    "## The Challenge: Built-in Tools and Sub-agents\n",
    "\n",
    "In ADK, you might think to use the built-in BigQuery toolset for database queries. It handles authentication, query execution, and results formatting automatically.\n",
    "\n",
    "However, there's an important limitation in the ADK documentation:\n",
    "\n",
    "> **\"Built-in tools cannot be used within a sub-agent\"**\n",
    "\n",
    "Since we need to orchestrate multiple agents working together (some in parallel), we can't use the built-in BigQuery toolset. Instead, we'll create a custom function tool that our agents can use.\n",
    "\n",
    "This actually gives us more control and makes the code simpler to understandâ€”perfect for learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vibn3hptvrd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "from google.adk.agents import Agent, ParallelAgent, SequentialAgent\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types\n",
    "\n",
    "# Initialize BigQuery client with proper authentication\n",
    "credentials, project = google.auth.default()\n",
    "bq_client = bigquery.Client(credentials=credentials, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lw1pke2dqsn",
   "metadata": {},
   "source": [
    "## Creating Our BigQuery Query Tool\n",
    "\n",
    "First, we'll create a custom function tool that all our agents can use to query BigQuery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1iins4bimw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic BigQuery function tool that all agents will use\n",
    "def execute_query(query: str) -> dict:\n",
    "    \"\"\"Execute a BigQuery SQL query and return results\n",
    "    \n",
    "    Args:\n",
    "        query: The SQL query to execute\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with status and data/error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute the query\n",
    "        result = bq_client.query(query).result()\n",
    "        \n",
    "        # Convert rows to list of dictionaries\n",
    "        rows = []\n",
    "        for row in result:\n",
    "            rows.append(dict(row))\n",
    "            \n",
    "        return {\"status\": \"success\", \"data\": rows}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "print(\"âœ… BigQuery query tool created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cwqkhs1am8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our model for all agents\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "print(f\"âœ… Using model: {GEMINI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cljtliaesjq",
   "metadata": {},
   "source": [
    "## Creating the Data Collection Agents\n",
    "\n",
    "Now let's create our agents. First, the order fetcher agent that gets the main delivery data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "azeeock1u9g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First agent - gets order data for a specific order number\n",
    "order_fetcher_agent = Agent(\n",
    "    model=GEMINI_MODEL,\n",
    "    name=\"order_fetcher_agent\",\n",
    "    description=\"Fetches order data for a specific order number\",\n",
    "    instruction=f\"\"\"\\\n",
    "        You will receive an order number in the user message.\n",
    "        Use the execute_query tool to run this SQL query:\n",
    "        \n",
    "        ```sql\n",
    "        SELECT *\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.delivery_orders`\n",
    "        WHERE CUSTOMER_ORDER_NUMBER = '<ORDER_NUMBER>'\n",
    "        ```\n",
    "        \n",
    "        Replace <ORDER_NUMBER> with the actual order number provided.\n",
    "        Return the order data found.\n",
    "        \"\"\",\n",
    "    tools=[execute_query],\n",
    "    output_key=\"order_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ooc692qxrqi",
   "metadata": {},
   "source": [
    "## Additional Data Agents\n",
    "\n",
    "Next, we create agents to fetch related data (customer notes and products). These will run in parallel for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j5z1u4uxe0m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer notes agent - gets special instructions\n",
    "customer_notes_agent = Agent(\n",
    "    model=GEMINI_MODEL,\n",
    "    name=\"customer_notes_agent\", \n",
    "    description=\"Fetches customer notes and summaries\",\n",
    "    instruction=f\"\"\"\\\n",
    "You will receive order information in {{order_data}}.\n",
    "Extract the DATA_ID value from that data.\n",
    "\n",
    "Use the execute_query tool to run this SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.delivery_notes`\n",
    "WHERE DATA_ID = <DATA_ID>\n",
    "```\n",
    "\n",
    "Replace <DATA_ID> with the actual DATA_ID value.\n",
    "Return the notes data found.\n",
    "\"\"\",\n",
    "    tools=[execute_query],\n",
    "    output_key=\"notes_data\"\n",
    ")\n",
    "\n",
    "# Products agent - gets SKU information\n",
    "products_agent = Agent(\n",
    "    model=GEMINI_MODEL,\n",
    "    name=\"products_agent\",\n",
    "    description=\"Fetches product information for the order\",\n",
    "    instruction=f\"\"\"\\\n",
    "You will receive order information in {{order_data}}.\n",
    "Extract the DATA_ID value from that data.\n",
    "\n",
    "Use the execute_query tool to run this SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.delivery_products`\n",
    "WHERE DATA_ID = <DATA_ID>\n",
    "```\n",
    "\n",
    "Replace <DATA_ID> with the actual DATA_ID value.\n",
    "Return all products associated with this order.\n",
    "\"\"\",\n",
    "    tools=[execute_query],\n",
    "    output_key=\"products_data\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Data fetching agents created\")\n",
    "print(\"\\nThese agents will:\")\n",
    "print(\"1. Fetch delivery notes (special instructions)\")\n",
    "print(\"2. Get all products/SKUs for the order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5645cd",
   "metadata": {},
   "source": [
    "## Orchestrating with Workflow Agents\n",
    "\n",
    "Now comes the magic of ADK's workflow agents. We'll use a `ParallelAgent` to run both customer and product queries at the same time.\n",
    "\n",
    "**ParallelAgent** benefits:\n",
    "- Faster execution (both queries run simultaneously)\n",
    "- Better resource utilization\n",
    "- Simpler error handling (if one fails, you know immediately)\n",
    "\n",
    "The parallel agent doesn't do any work itselfâ€”it just coordinates the execution of its sub-agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel agent for notes and products (after we have the order)\n",
    "parallel_data_agent = ParallelAgent(\n",
    "    name=\"parallel_data_agent\",\n",
    "    sub_agents=[customer_notes_agent, products_agent],\n",
    "    description=\"Fetch customer notes and products in parallel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kjnqs3nc0bn",
   "metadata": {},
   "source": [
    "## Data Assembly Agent\n",
    "\n",
    "The final agent assembles all the collected data into a structured format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6925wbap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final agent that assembles the structured data\n",
    "data_assembler_agent = Agent(\n",
    "    name=\"data_assembler_agent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    description=\"Assembles all collected data into structured format\",\n",
    "    instruction=\"\"\"\\\n",
    "You have data from the previous agents:\n",
    "Order Data: {{order_data}}\n",
    "Notes Data: {{notes_data}}  \n",
    "Products Data: {{products_data}}\n",
    "\n",
    "Create a structured JSON output combining all data in this format:\n",
    "{{\n",
    "    \"order\": {{\n",
    "        \"DATA_ID\": <from order data>,\n",
    "        \"CUSTOMER_ORDER_NUMBER\": <from order data>,\n",
    "        \"WORK_ORDER_NUMBER\": <from order data>,\n",
    "        \"SCHEDULED_DELIVERY_DATE\": <from order data>,\n",
    "        \"VEHICLE_TYPE\": <from order data>,\n",
    "        \"QUANTITY\": <from order data>,\n",
    "        \"VOLUME_CUBEFT\": <from order data>,\n",
    "        \"WEIGHT\": <from order data>,\n",
    "        \"PALLET\": <from order data>,\n",
    "        \"SERVICE_TYPE\": <from order data>,\n",
    "        \"WINDOW_START\": <from order data>,\n",
    "        \"WINDOW_END\": <from order data>\n",
    "    }},\n",
    "    \"customer\": {{\n",
    "        \"CUSTOMER_NAME\": <from order data>,\n",
    "        \"PRO_XTRA_MEMBER\": <from order data>,\n",
    "        \"MANAGED_ACCOUNT\": <from order data>,\n",
    "        \"COMMERCIAL_ADDRESS_FLAG\": <from order data>,\n",
    "        \"DESTINATION_ADDRESS\": <from order data>,\n",
    "        \"BUSINESS_HOURS\": <from order data>,\n",
    "        \"CUSTOMER_NOTES\": <from notes data>,\n",
    "        \"CUSTOMER_NOTES_LLM_SUMMARY\": <from notes data>\n",
    "    }},\n",
    "    \"products\": [<list of SKU_DESCRIPTION from products data>],\n",
    "    \"environmental\": {{\n",
    "        \"WTHR_CATEGORY\": <from order data>,\n",
    "        \"PRECIPITATION\": <from order data>,\n",
    "        \"STRT_VW_IMG_DSCRPTN\": <from order data>\n",
    "    }},\n",
    "    \"risk_info\": {{\n",
    "        \"DLVRY_RISK_DECILE\": <from order data>,\n",
    "        \"DLVRY_RISK_BUCKET\": <from order data>,\n",
    "        \"DLVRY_RISK_PERCENTILE\": <from order data>,\n",
    "        \"DLVRY_RISK_TOP_FEATURE\": <from order data>\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON structure, no additional text.\n",
    "\"\"\",\n",
    "    tools=[],\n",
    "    output_key=\"structured_data\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Data assembler agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd2582a",
   "metadata": {},
   "source": [
    "## Building the Complete Pipeline\n",
    "\n",
    "Now we bring it all together with a `SequentialAgent` that orchestrates our entire flow:\n",
    "\n",
    "1. **order_fetcher_agent** â†’ Gets data for our specific order\n",
    "2. **parallel_data_agent** â†’ Runs notes & products agents in parallel\n",
    "3. **data_assembler_agent** â†’ Combines everything into structured JSON\n",
    "\n",
    "This creates a reusable pipeline that can fetch data for any order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data collection pipeline\n",
    "data_collection_pipeline = SequentialAgent(\n",
    "    name=\"data_collection_pipeline\",\n",
    "    sub_agents=[order_fetcher_agent, parallel_data_agent, data_assembler_agent],\n",
    "    description=\"Complete data collection pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f53f35",
   "metadata": {},
   "source": [
    "## Setting Up the ADK Runtime\n",
    "\n",
    "Before we can run our agents, we need to set up the ADK runtime components:\n",
    "\n",
    "1. **Session Service**: Manages conversation state and agent outputs\n",
    "2. **Runner**: Executes the agent pipeline and handles the event stream\n",
    "\n",
    "Let's configure these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "APP_NAME = \"data_collection\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"data_session_001\"\n",
    "\n",
    "# Create session service\n",
    "session_service = InMemorySessionService()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41371e",
   "metadata": {},
   "source": [
    "## Running the Pipeline\n",
    "\n",
    "Now for the exciting part! Let's run our complete data collection pipeline.\n",
    "\n",
    "We'll create an async function that:\n",
    "1. Creates a session\n",
    "2. Sets up the runner\n",
    "3. Sends a message to start the pipeline\n",
    "4. Processes the event stream, showing progress\n",
    "5. Displays only the final formatted output\n",
    "\n",
    "Notice how we suppress warnings to keep the output clean for learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "async def collect_data(order_number: str):\n",
    "    \"\"\"Collect data for a specific order\"\"\"\n",
    "    # Create session\n",
    "    await session_service.create_session(\n",
    "        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    "    )\n",
    "\n",
    "    # Create runner\n",
    "    runner = Runner(\n",
    "        agent=data_collection_pipeline,\n",
    "        app_name=APP_NAME,\n",
    "        session_service=session_service,\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA COLLECTION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nCollecting data for order: {order_number}\")\n",
    "    print(\"\\nRunning data collection pipeline...\\n\")\n",
    "\n",
    "    # Create the user message with the order number\n",
    "    content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=order_number)],\n",
    "    )\n",
    "\n",
    "    # Temporarily redirect stderr to suppress runtime warnings\n",
    "    old_stderr = sys.stderr\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        async for event in runner.run_async(\n",
    "            user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
    "        ):\n",
    "            # Restore stderr briefly for our print statements\n",
    "            sys.stderr = old_stderr\n",
    "            \n",
    "            # Show progress\n",
    "            if hasattr(event, \"author\") and event.author:\n",
    "                if event.author in [\n",
    "                    \"order_fetcher_agent\",\n",
    "                    \"customer_notes_agent\", \n",
    "                    \"products_agent\",\n",
    "                    \"data_assembler_agent\"\n",
    "                ]:\n",
    "                    print(f\"[{event.author}] processing...\")\n",
    "\n",
    "            # Capture final structured output\n",
    "            if event.is_final_response() and event.author == \"data_assembler_agent\":\n",
    "                if event.content and event.content.parts:\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(\"DATA COLLECTION COMPLETE\")\n",
    "                    print(\"=\" * 60)\n",
    "                    \n",
    "                    # Parse and save JSON\n",
    "                    try:\n",
    "                        data_text = event.content.parts[0].text.strip()\n",
    "                        # Remove markdown if present\n",
    "                        if data_text.startswith(\"```json\"):\n",
    "                            data_text = data_text[7:]\n",
    "                        if data_text.endswith(\"```\"):\n",
    "                            data_text = data_text[:-3]\n",
    "                        \n",
    "                        json_data = json.loads(data_text.strip())\n",
    "                        \n",
    "                        # Save to file\n",
    "                        with open('collected_order_data.json', 'w') as f:\n",
    "                            json.dump(json_data, f, indent=2)\n",
    "                        \n",
    "                        # Display summary\n",
    "                        print(f\"\\nOrder: {json_data['order']['CUSTOMER_ORDER_NUMBER']}\")\n",
    "                        print(f\"Customer: {json_data['customer']['CUSTOMER_NAME']}\")\n",
    "                        print(f\"Delivery Date: {json_data['order']['SCHEDULED_DELIVERY_DATE']}\")\n",
    "                        print(f\"Products: {len(json_data['products'])} items\")\n",
    "                        print(f\"Risk Level: {json_data['risk_info']['DLVRY_RISK_BUCKET']}\")\n",
    "                        \n",
    "                        print(\"\\nâœ… Data saved to collected_order_data.json\")\n",
    "                        print(\"\\nThis file will be used by subsequent exercises.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing JSON: {e}\")\n",
    "                        print(event.content.parts[0].text)\n",
    "                break\n",
    "                \n",
    "            # Suppress warnings again\n",
    "            sys.stderr = io.StringIO()\n",
    "    finally:\n",
    "        # Always restore stderr\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Run the pipeline with our selected order\n",
    "await collect_data(ORDER_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urwwx2798r",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "Let's break down the execution flow:\n",
    "\n",
    "1. **order_fetcher_agent** ran first and fetched all data for order `{ORDER_NUMBER}` from BigQuery\n",
    "2. Its output was saved as `order_data`\n",
    "3. **parallel_data_agent** orchestrated two parallel queries:\n",
    "   - **customer_notes_agent** extracted DATA_ID and fetched notes\n",
    "   - **products_agent** extracted DATA_ID and fetched all products\n",
    "4. Both outputs were saved as `notes_data` and `products_data`\n",
    "5. **data_assembler_agent** combined all three outputs into structured JSON\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "- **Real Data**: We're now using actual delivery data from BigQuery\n",
    "- **Order-Specific**: The pipeline works with any order number\n",
    "- **Structured Output**: Data is saved as `collected_order_data.json` for use in subsequent exercises\n",
    "- **Reusable**: This same order will flow through all workshop exercises\n",
    "\n",
    "This demonstrates the power of ADK's agent orchestration for building data pipelines that work with production data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vyjofmxcgz",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Now it's your turn to experiment with the code. Here are some exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Get Multiple Orders\n",
    "Modify the `order_selector_agent` to fetch the 5 most recent orders instead of just 1. How would you need to change the downstream agents?\n",
    "\n",
    "### Exercise 2: Add a New Data Source\n",
    "Create a new agent that fetches delivery status information. Add it to the parallel execution group. You'll need to:\n",
    "- Create the agent with appropriate SQL\n",
    "- Add it to the `related_data_agent` sub_agents list\n",
    "- Update the display agent to show the new data\n",
    "\n",
    "### Exercise 3: Custom Formatting\n",
    "Modify the display agent to output data in a different format (e.g., JSON, CSV-style, or a more detailed report).\n",
    "\n",
    "### Challenge: Error Handling\n",
    "What happens if a customer ID doesn't exist? Add error handling to make the pipeline more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7ywjontfzo",
   "metadata": {},
   "source": [
    "## Troubleshooting & Key Concepts\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **\"Built-in tools cannot be used within a sub-agent\"**\n",
    "   - This is why we created custom function tools\n",
    "   - Always use custom tools when building multi-agent workflows\n",
    "\n",
    "2. **Token Limit Errors**\n",
    "   - Keep queries focused and limit result sizes\n",
    "   - Use `LIMIT` clauses in SQL queries\n",
    "   - Consider pagination for large datasets\n",
    "\n",
    "3. **Agent Communication**\n",
    "   - Always specify `output_key` for agents whose data you need later\n",
    "   - Use `{output_key_name}` in instructions to reference previous outputs\n",
    "   - Check agent names match when filtering events\n",
    "\n",
    "4. **Warning Messages**\n",
    "   - We suppress warnings for cleaner educational output\n",
    "   - In production, you might want to see these for debugging\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Custom tools** provide flexibility when built-in tools have limitations\n",
    "- **Sequential and Parallel agents** let you orchestrate complex workflows\n",
    "- **output_key** enables data passing between agents\n",
    "- **Event processing** lets you control what users see\n",
    "- **Generic functions** reduce code duplication and improve maintainability\n",
    "\n",
    "Congratulations! You've built a sophisticated data orchestration pipeline using Google ADK. This foundation will serve you well as you build more complex agent systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
